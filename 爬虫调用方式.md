# Newsletter 爬虫调用方式

本文档介绍 Newsletter 爬虫系统的使用方法和调用方式。

## 快速开始

### 1. 安装依赖

```bash
# 安装 Python 依赖
pip install -r requirements.txt

# 安装 Playwright 浏览器驱动（必需）
playwright install chromium
```

### 2. 基本使用

```bash
# 爬取所有文章
python3 main.py crawl

# 上传到 OSS/MinIO
python3 main.py upload

# 指定自定义 bucket
python3 main.py upload --bucket my-bucket-name
```

## 命令详解

### 爬取命令 (crawl)

爬取 Newsletter 网站的所有文章，转换为 Markdown 格式并下载图片。

```bash
python3 main.py crawl
```

**功能特性：**
- 自动获取所有文章列表（目前约 180 篇）
- 批量处理文章（默认 5 篇/批）
- 并发下载图片
- 自动转换 HTML 为 Markdown
- 保存文章元数据

**输出目录结构：**
```
crawled_data/
├── articles/           # 文章目录
│   ├── {id}_{slug}/   # 每篇文章的独立目录
│   │   ├── content.md # Markdown 内容
│   │   ├── metadata.json # 元数据
│   │   └── images/    # 文章图片
│   │       ├── cover.jpg
│   │       └── img_*.jpg
├── images/            # 封面图片（已废弃）
└── data/              # 数据文件
    ├── articles_metadata.json    # 原始元数据
    ├── processed_articles.json   # 处理后的完整数据
    ├── recommendation_data.json  # 推荐系统数据
    └── progress.json             # 进度文件
```

### 上传命令 (upload)

将爬取的文章上传到 MinIO/OSS 对象存储。

```bash
# 使用默认配置上传
python3 main.py upload

# 指定 bucket 名称
python3 main.py upload --bucket newsletter-backup

# 不使用断点续传
python3 main.py upload --no-resume
```

**上传内容：**
- Markdown 文件
- 图片文件
- 元数据 JSON

**对象存储路径：**
```
bucket/
└── articles/
    └── nlp-elvissaravia/
        └── {article_id}/
            ├── content.md
            ├── metadata.json
            └── images/
                └── *.jpg
```

## 配置文件

### config.json

主要配置参数：

```json
{
  "crawler": {
    "base_url": "https://nlp.elvissaravia.com",
    "api_endpoint": "/api/v1/archive",
    "output_directory": "crawled_data",
    "request_delay": 2,           // API 请求延迟（秒）
    "max_concurrent_articles": 2, // 最大并发文章数
    "max_concurrent_images": 5,   // 最大并发图片下载数
    "batch_size": 5,              // 批处理大小
    "api_delay": 1.0,             // API 请求间隔
    "article_delay": 0.5,         // 文章处理间隔
    "enable_resume": false        // 是否启用断点续传
  },
  "oss": {
    "base_url": "http://localhost:9011",
    "public_base_url": "http://60.205.160.74:9000",
    "bucket_name": "newsletter-articles-nlp",
    "source_id": "nlp-elvissaravia",
    "max_concurrent_uploads": 10,
    "upload_timeout": 60,
    "retry_attempts": 3
  }
}
```

### 环境变量

```bash
# MinIO/OSS 认证（必需）
export MINIO_ACCESS_KEY="your-access-key"
export MINIO_SECRET_KEY="your-secret-key"

# 可选：覆盖配置文件中的 endpoint
export MINIO_ENDPOINT="http://localhost:9000"
```

## 高级用法

### 1. 断点续传

爬虫支持自动保存进度，中断后可继续：

```bash
# 启用断点续传（修改 config.json）
"enable_resume": true

# 或强制重新开始
rm -rf crawled_data/data/progress.json
python3 main.py crawl
```

### 2. 自定义输出目录

```bash
# 修改 config.json
"output_directory": "my_custom_output"

# 或在代码中指定
config.crawler.output_dir = "test_crawl"
```

### 3. 调整性能参数

```json
{
  "crawler": {
    "max_concurrent_articles": 3,  // 增加并发（注意资源消耗）
    "max_concurrent_images": 10,   // 增加图片并发
    "batch_size": 10,              // 增大批处理
    "article_delay": 0.2           // 减少延迟（注意限流）
  }
}
```

### 4. Python 代码调用

```python
import asyncio
from src.newsletter_system.crawler.newsletter_crawler import NewsletterCrawler
from src.newsletter_system.config.settings import load_config

async def crawl_articles():
    # 加载配置
    config = load_config()
    
    # 创建爬虫实例
    crawler = NewsletterCrawler(config.crawler)
    
    # 使用上下文管理器
    async with crawler:
        # 获取所有文章元数据
        metadata = await crawler.get_all_articles_metadata()
        print(f"获取到 {len(metadata)} 篇文章")
        
        # 批量处理文章
        for i in range(0, len(metadata), config.crawler.batch_size):
            batch = metadata[i:i+config.crawler.batch_size]
            results = await crawler.process_article_batch(batch)
            print(f"处理了 {len(results)} 篇文章")

# 运行爬虫
asyncio.run(crawl_articles())
```

### 5. 单篇文章爬取

```python
async def crawl_single_article():
    config = load_config()
    crawler = NewsletterCrawler(config.crawler)
    
    # 文章信息
    article = {
        'id': 164818220,
        'title': 'Top AI Papers of the Week',
        'canonical_url': 'https://nlp.elvissaravia.com/p/...',
        'slug': 'top-ai-papers-of-the-week',
        # ... 其他字段
    }
    
    async with crawler:
        results = await crawler.process_article_batch([article])
        if results:
            print("爬取成功")
```

## 常见问题

### 1. ERR_ABORTED 错误

**问题**：大量文章出现 `Page.goto: net::ERR_ABORTED` 错误

**解决方案**：
- 降低并发数：`"max_concurrent_articles": 2`
- 减小批处理大小：`"batch_size": 5`
- 增加延迟：`"article_delay": 1.0`

### 2. 429 限流错误

**问题**：检测到限流 (HTTP 429)

**解决方案**：
- 增加请求延迟：`"request_delay": 3`
- 减少并发：`"max_concurrent_articles": 1`
- 使用断点续传，避免重复请求

### 3. 图片下载失败

**问题**：`Cannot connect to host substackcdn.com:443`

**原因**：网络连接问题或 CDN 限制

**解决方案**：
- 检查网络连接
- 使用代理（如需要）
- 图片下载失败不影响文章内容

### 4. 内容为空

**问题**：文章目录创建但 content.md 为空

**解决方案**：
- 检查页面选择器是否正确
- 查看日志中的具体错误
- 手动测试单篇文章

## 性能优化建议

1. **合理设置并发数**
   - 推荐：`max_concurrent_articles: 2-3`
   - 过高会导致浏览器崩溃

2. **批处理大小**
   - 推荐：`batch_size: 5-10`
   - 根据内存和网络情况调整

3. **使用断点续传**
   - 避免重复爬取
   - 支持中断恢复

4. **定期清理**
   ```bash
   # 清理进度文件
   rm -f crawled_data/data/progress.json
   
   # 清理所有数据重新爬取
   rm -rf crawled_data
   ```

## 监控和日志

### 查看进度

```bash
# 实时查看日志
tail -f crawled_data/crawler.log

# 统计结果
echo "文章总数: $(ls -d crawled_data/articles/*/ | wc -l)"
echo "有内容文章: $(find crawled_data/articles -name "content.md" -size +0 | wc -l)"
echo "图片总数: $(find crawled_data/articles -path "*/images/*" -type f | wc -l)"
```

### 检查失败文章

```bash
# 查找空内容文章
find crawled_data/articles -name "content.md" -size 0

# 查找没有内容文件的目录
for dir in crawled_data/articles/*/; do
  [ ! -f "$dir/content.md" ] && echo "$dir"
done
```

## 注意事项

1. **首次运行**需要安装 Playwright 浏览器驱动
2. **完整爬取**约需 5-10 分钟（取决于网络）
3. **磁盘空间**：完整数据约需 500MB
4. **内存使用**：建议至少 2GB 可用内存
5. **网络要求**：稳定的网络连接，支持 HTTPS

## 技术架构

- **异步并发**：基于 asyncio 和 aiohttp
- **浏览器自动化**：Playwright (Chromium)
- **HTML 转换**：markdownify
- **对象存储**：MinIO Python SDK
- **进度管理**：JSON 文件持久化

## 更新日志

### 最新修复 (2024-08)
- ✅ 修复 429 误判问题
- ✅ 修复登录检测过于敏感
- ✅ 解决 ERR_ABORTED 并发问题
- ✅ 优化批处理和页面池管理
- ✅ 添加信号量控制并发访问