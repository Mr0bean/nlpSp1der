#!/usr/bin/env python3
"""
çˆ¬è™«æ€§èƒ½å¯¹æ¯”è„šæœ¬
"""

import asyncio
import sys
import time
from pathlib import Path
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.newsletter_crawler import NewsletterCrawler
from crawler.optimized_crawler import OptimizedNewsletterCrawler, CrawlerConfig


async def benchmark_basic_crawler():
    """æµ‹è¯•åŸºç¡€çˆ¬è™«æ€§èƒ½"""
    print("ğŸ” æµ‹è¯•åŸºç¡€çˆ¬è™«...")
    
    start_time = time.time()
    
    try:
        async with NewsletterCrawler(output_dir="test_basic") as crawler:
            # åªè·å–å‰5ç¯‡æ–‡ç« è¿›è¡Œæµ‹è¯•
            articles_metadata = await crawler.get_all_articles_metadata()
            test_articles = articles_metadata[:5]
            
            processed_count = 0
            for article in test_articles:
                try:
                    await crawler.process_article(article)
                    processed_count += 1
                except Exception as e:
                    print(f"å¤„ç†æ–‡ç« å¤±è´¥: {e}")
            
            elapsed = time.time() - start_time
            
            return {
                'name': 'åŸºç¡€çˆ¬è™«',
                'processed_articles': processed_count,
                'total_time': elapsed,
                'avg_time_per_article': elapsed / processed_count if processed_count > 0 else 0
            }
    except Exception as e:
        print(f"åŸºç¡€çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return None


async def benchmark_optimized_crawler():
    """æµ‹è¯•ä¼˜åŒ–çˆ¬è™«æ€§èƒ½"""
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ–çˆ¬è™«...")
    
    start_time = time.time()
    
    try:
        config = CrawlerConfig(
            output_dir="test_optimized",
            max_concurrent_articles=3,
            max_concurrent_images=10,
            batch_size=5,
            enable_resume=False
        )
        
        async with OptimizedNewsletterCrawler(config) as crawler:
            # åªè·å–å‰5ç¯‡æ–‡ç« è¿›è¡Œæµ‹è¯•
            articles_metadata = await crawler.get_all_articles_metadata()
            test_articles = articles_metadata[:5]
            
            # å¤„ç†æ–‡ç« 
            results = await crawler.process_article_batch(test_articles)
            processed_count = len([r for r in results if r is not None])
            
            elapsed = time.time() - start_time
            
            return {
                'name': 'ä¼˜åŒ–çˆ¬è™«',
                'processed_articles': processed_count,
                'total_time': elapsed,
                'avg_time_per_article': elapsed / processed_count if processed_count > 0 else 0
            }
    except Exception as e:
        print(f"ä¼˜åŒ–çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return None


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š çˆ¬è™«æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•åŸºç¡€çˆ¬è™«
    basic_result = await benchmark_basic_crawler()
    
    print("\n" + "=" * 50)
    
    # æµ‹è¯•ä¼˜åŒ–çˆ¬è™«
    optimized_result = await benchmark_optimized_crawler()
    
    print("\n" + "=" * 50)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 50)
    
    if basic_result:
        print(f"ğŸ” {basic_result['name']}:")
        print(f"   - å¤„ç†æ–‡ç« æ•°: {basic_result['processed_articles']}")
        print(f"   - æ€»è€—æ—¶: {basic_result['total_time']:.2f}ç§’")
        print(f"   - å¹³å‡æ¯ç¯‡: {basic_result['avg_time_per_article']:.2f}ç§’")
        print()
    
    if optimized_result:
        print(f"ğŸš€ {optimized_result['name']}:")
        print(f"   - å¤„ç†æ–‡ç« æ•°: {optimized_result['processed_articles']}")
        print(f"   - æ€»è€—æ—¶: {optimized_result['total_time']:.2f}ç§’")
        print(f"   - å¹³å‡æ¯ç¯‡: {optimized_result['avg_time_per_article']:.2f}ç§’")
        print()
    
    if basic_result and optimized_result:
        if optimized_result['total_time'] > 0:
            speedup = basic_result['total_time'] / optimized_result['total_time']
            print(f"âš¡ æ€§èƒ½æå‡: {speedup:.2f}x")
            
            efficiency_improvement = (basic_result['avg_time_per_article'] - optimized_result['avg_time_per_article']) / basic_result['avg_time_per_article'] * 100
            print(f"ğŸ“Š æ•ˆç‡æå‡: {efficiency_improvement:.1f}%")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("- å¯¹äºå¤§è§„æ¨¡çˆ¬å–ï¼Œæ¨èä½¿ç”¨ä¼˜åŒ–ç‰ˆçˆ¬è™«")
    print("- å¯ä»¥é€šè¿‡è°ƒæ•´å¹¶å‘å‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½")
    print("- å¯ç”¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½å¯ä»¥é¿å…é‡å¤å·¥ä½œ")


if __name__ == "__main__":
    asyncio.run(main())