#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çˆ¬è™« - çˆ¬å–10ç¯‡æ–‡æ¡£
"""

import asyncio
import sys
import time
from pathlib import Path
import json

# æ·»åŠ  src åˆ° Python è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parents[1]))  # æŒ‡å‘ src ç›®å½•

from newsletter_system.crawler.newsletter_crawler import NewsletterCrawler, CrawlerConfig


async def test_crawler_10_articles():
    """æµ‹è¯•çˆ¬å–10ç¯‡æ–‡æ¡£"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•çˆ¬è™« - çˆ¬å–10ç¯‡æ–‡æ¡£")
    print("=" * 50)
    
    # é…ç½®çˆ¬è™«
    config = CrawlerConfig(
        output_dir="test_crawl_10",
        max_concurrent_articles=3,
        max_concurrent_images=8, 
        batch_size=5,
        enable_resume=True,
        api_delay=0.5,
        article_delay=0.3
    )
    
    start_time = time.time()
    
    try:
        async with NewsletterCrawler(config) as crawler:
            # è·å–æ–‡ç« åˆ—è¡¨
            print("ğŸ“‹ è·å–æ–‡ç« åˆ—è¡¨...")
            articles_metadata = await crawler.get_all_articles_metadata()
            
            if not articles_metadata:
                print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« ")
                return
            
            # åªå¤„ç†å‰10ç¯‡
            test_articles = articles_metadata[:10]
            print(f"ğŸ“„ å‡†å¤‡å¤„ç† {len(test_articles)} ç¯‡æ–‡ç« ")
            
            # ä¿å­˜æµ‹è¯•ç”¨çš„å…ƒæ•°æ®
            metadata_file = Path(config.output_dir) / "data" / "test_metadata.json"
            metadata_file.parent.mkdir(parents=True, exist_ok=True)
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(test_articles, f, ensure_ascii=False, indent=2)
            
            # åˆ†æ‰¹å¤„ç†
            processed_articles = []
            recommendation_data = []
            
            for i in range(0, len(test_articles), config.batch_size):
                batch = test_articles[i:i + config.batch_size]
                batch_num = i // config.batch_size + 1
                total_batches = (len(test_articles) + config.batch_size - 1) // config.batch_size
                
                print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ç¯‡æ–‡ç« )")
                
                batch_results = await crawler.process_article_batch(batch)
                
                for result in batch_results:
                    if result:
                        processed_articles.append(result['article_data'])
                        recommendation_data.append(result['recommendation_data'])
                        print(f"âœ… å®Œæˆ: {result['article_data'].get('title', 'Unknown')[:50]}...")
                
                print(f"ğŸ“Š æ‰¹æ¬¡å®Œæˆï¼Œå·²å¤„ç† {len(processed_articles)} ç¯‡")
            
            # ä¿å­˜ç»“æœ
            data_dir = Path(config.output_dir) / "data"
            
            # ä¿å­˜å¤„ç†åçš„æ–‡ç« 
            processed_file = data_dir / "processed_articles.json"
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump(processed_articles, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ¨èæ•°æ®
            recommendation_file = data_dir / "recommendation_data.json"
            with open(recommendation_file, 'w', encoding='utf-8') as f:
                json.dump(recommendation_data, f, ensure_ascii=False, indent=2)
            
            elapsed_time = time.time() - start_time
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_images = sum(len(article.get('local_images', [])) for article in processed_articles)
            
            print("\n" + "=" * 50)
            print("ğŸ“Š æµ‹è¯•å®Œæˆç»Ÿè®¡:")
            print("=" * 50)
            print(f"ğŸ¯ ç›®æ ‡æ–‡ç« æ•°: 10")
            print(f"âœ… æˆåŠŸå¤„ç†: {len(processed_articles)}")
            print(f"âŒ å¤±è´¥æ•°é‡: {len(crawler.progress.failed_articles)}")
            print(f"ğŸ–¼ï¸  ä¸‹è½½å›¾ç‰‡: {len(crawler.progress.downloaded_images)}")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"âš¡ å¹³å‡æ¯ç¯‡: {elapsed_time/len(processed_articles):.2f} ç§’" if processed_articles else "N/A")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
            
            if crawler.progress.failed_articles:
                print(f"\nâŒ å¤±è´¥æ–‡ç« :")
                for article_id, error in crawler.progress.failed_articles.items():
                    print(f"   - æ–‡ç«  {article_id}: {error}")
            
            # å±•ç¤ºæˆåŠŸå¤„ç†çš„æ–‡ç« 
            if processed_articles:
                print(f"\nâœ… æˆåŠŸå¤„ç†çš„æ–‡ç« :")
                for i, article in enumerate(processed_articles[:5], 1):
                    title = article.get('title', 'Unknown')[:60]
                    images_count = len(article.get('local_images', []))
                    print(f"   {i}. {title}... ({images_count} å›¾ç‰‡)")
                
                if len(processed_articles) > 5:
                    print(f"   ... è¿˜æœ‰ {len(processed_articles) - 5} ç¯‡")
            
            return {
                'processed': len(processed_articles),
                'failed': len(crawler.progress.failed_articles),
                'images': len(crawler.progress.downloaded_images),
                'time': elapsed_time
            }
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


async def main():
    """ä¸»å‡½æ•°"""
    result = await test_crawler_10_articles()
    
    if result:
        print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸå®Œæˆ!")
        print(f"æ€§èƒ½æŒ‡æ ‡: {result['processed']}/10 ç¯‡æ–‡ç« ï¼Œ{result['time']:.1f}ç§’")
    else:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(main())